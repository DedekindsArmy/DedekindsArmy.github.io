---
title: Shannon's entropy - Introduction to information theory
author: Jsevillamol
---
**Lecturer**: Jaime Sevilla

**Date**: 10/10/2017

**Time**: 17:00

**Place**: Room 104

**Abstract**:

Entropy is a versatile tool which helps the study and analysis of diverse areas.

In this talk we will axiomatically derive the definition of entropy, and understand some of its more common interpretations and properties.

We will then use our newfound knowledge to study the relation between thermodynamical and Shannon's entropy, and its application to communication, data compression and investment.

Lastly, we will dip our toes in the advanced topic of algorithmic complexity, which further generalizes the concept of Shannon's entropy to allow an schema of universal reasoning.

Keywords: Markov process interpretation of thermodynamic entropy, Kelly gambling, the noisy channel theorem, mutual information, relative entropy, kolmogorov complexity.

## Slides
[Slides about information theory in Prezi](https://prezi.com/p/z2bu2nonjg3y/)

## Bibliography

* T. Cover, J. Thomas, *Elements of Information Theory*
* D. Mackay, [*Information Theory, Inference, and Learning Algorithms*](http://www.inference.org.uk/itprnn/book.pdf)


## Promotional poster
 <img src="https://document-export.canva.com/DACiEMYbOn4/29/preview/0001-375821527.png" alt="Poster" style="width: 750px;"/>
